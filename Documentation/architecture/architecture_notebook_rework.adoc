= Architecture Notebook: Follow-Me Fahrroboter
{localdatetime}
include::../_includes/default-attributes.inc.adoc[]
// Platzhalter für weitere Dokumenten-Attribute

== Zweck
Dieses Dokument beschreibt die Philosophie, Entscheidungen, Nebenbedingungen, Begründungen, wesentliche Elemente und andere übergreifende Aspekte des Systems, die Einfluss auf Entwurf und Implementierung haben.

== Architekturziele und Philosophie
Unsere Philosophie in diesem Projekt ist es, das bestmögliche Ergebnis in gegebener Zeit zu erzielen und das Projekt so modular wie möglich zu gestalten. Übersichtlichkeit und Erweiterbarkeit sind unsere großen Ziele. Unsere Teamphilosophie ist es, aufkommende Aufgaben so schnell und bestmöglich für die jeweilige Iteration zu erledigen und einen funktionsfähigen Prototypen zu entwickeln. 
Besondere Herausforderungen sind: 

* Die Integration mit bestehenden Systemen (z.B. ROS 2). 
* Die Berücksichtigung von Hardware-Einschränkungen (z.B. Raspberry Pi und Alphabot 2).

== Annahmen und Abhängigkeiten
Es gibt zwei ausschlaggebende Punkte, die unsere Architektur stark beeinflussen:

1. *ROS 2*: Wir haben die Vorgabe erhalten, ROS 2 zu verwenden. ROS 2 bringt einige Eigenheiten mit sich, da es als eine Art Betriebssystem funktioniert. ROS 2 kommt mit einer Paketstruktur, innerhalb derer sich Nodes befinden, die den Code enthalten. Diese Nodes können systemweit mithilfe von Messages interagieren. Wir sind somit an diese Strukturvorgaben gebunden.

2. *Hardware-Limitierungen Prototyp 1*: 
    - *Raspberry Pi 4 8GB*: Der Raspberry Pi ist ein kostengünstiger, kleiner Computer, der vielseitig einsetzbar und einfach zu bedienen ist. Er weist jedoch Leistungsgrenzen auf, die Entscheidungen unter Berücksichtigung dieser Performance-Limitierungen erforderlich machen.
    - *Alphabot 2*: Der Alphabot 2 ist für die Verwendung mit dem Raspberry Pi konzipiert und enthält Komponenten wie Servo- und DC-Motoren, PCA9685, LEDs, Joystick, IR-Fernbedienung, Line-Sensoren und eine Kamera. Einschränkungen bestehen aufgrund der begrenzten Batterielebensdauer, der engen Integration und der veralteten Software.

3. *Hardware-Limitierungen Prototyp 2*: 
    - *Jetson Nano*: Der Jetson Nano ist ein etwas teurerer, aber leistungsstärkerer kleiner Computer, der vielseitig einsetzbar und einfach zu bedienen ist. Auch er weist Leistungsgrenzen auf, die Entscheidungen unter Berücksichtigung dieser Performance-Limitierungen erforderlich machen.
    - *Arduino*: Das Arduino bietet eine Vielzahl an Erweiterungs- und Einsatzmöglichkeiten, ist jedoch dadurch limitiert, dass es kein Threading unterstützt. Ressourcenmanagement ist daher bei der Verwendung besonders wichtig.


== Architektur-relevante Anforderungen
=== Benutzbarkeit (Usability)
    * NFAU-2: Das System soll als portable Möglichkeit zur Mitführung gestaltet werden

=== Zuverlässigkeit (Reliability)
    * NFAR-1: Das System sollte möglichst eine Stunde mit vollgeladenem Akku laufen
    * NFAR-2: Der Roboter sollte in der Lage sein, durch eine handelsübliche Powerbank, sobald dieser an den Zusatzakku angeschlossen wurde, in Betrieb genommen zu werden.
    * NFAR-3: Bei einem Fehler soll das System einfach durch Error-Handling in der Lage sein, sich nicht vollständig zu beenden, sondern einen Fehler zu werfen und so weiterhin zu laufen.
    
=== Leistung (Performance)
    * NFAP-1: Die Antwortzeit der Bilderkennung soll schnell genug sein, um Personen folgen zu können. Eine Verarbeitungszeit muss hier bei < 1 Sekunde sein.
    * NFAP-3: Der Alphabot 2 muss in der Lage sein, Bewegungsdaten mit einer Rate von ca. 100-1000 Datenpaketen pro Sekunde zu verarbeiten.
    * NFAP-4: Der Alphabot 2 samt Software muss in der Lage sein, innerhalb von 10 Minuten zu starten und innerhalb von 1 Minute sich vollständig zu beenden.
    
=== Wartbarkeit (Supportability)
    * NFAS-1: Durch die Integration mit dem OS ROS II, muss das System in der Lage sein, mit anderen Systemen kompatibel zu sein.

== Entscheidungen, Nebenbedingungen und Begründungen

1. *Verwendung von ROS 2 Humble*:
Wir haben uns für die Nutzung von ROS 2 Humble entschieden. Diese Version von ROS 2 bietet eine Vielzahl an Softwarelösungen sowie eine umfassende Dokumentation. Jedoch ist ROS 2 generell sehr komplex und erfordert eine lange Einarbeitungszeit.

2. *Verwendung von Ubuntu Server 22.04.03 LTS (64-bit)*:
Als Betriebssystem haben wir Ubuntu Server 22.04.03 LTS (64-bit) ausgewählt, da es das empfohlene Betriebssystem für ROS 2 Humble ist. Es ist mit vielerlei Software kompatibel und gehört zu den aktuellsten Ubuntu-Versionen. Ein Nachteil besteht jedoch darin, dass es keine grafische Benutzeroberfläche bietet.

3. *Verwendung von Python als Programmiersprache*:
Für die Programmierung verwenden wir Python. Python ist eine der beiden Sprachen, die mit ROS 2 kompatibel sind, und lässt sich einfacher implementieren als C++. Allerdings können in ROS 2 mit Python keine Message-Formate erstellt werden, sodass man auf die Standardformate angewiesen ist.

4. *Verwendung von CV Bridge*:
Die CV Bridge wird genutzt, um eine einfache Umwandlung des ROS 2 Image-Formats in das OpenCV Image-Format zu ermöglichen.

5. *Hinzufügen einer Web-Oberfläche*:
Zur Anzeige der vom Human-Detector bearbeiteten Bilder und für das notwendige Debugging haben wir eine Web-Oberfläche integriert. Dies führt jedoch zu einem Performance-Verlust.

6. *Verwendung von Flask für das Web-Tool*:
Für das Web-Tool haben wir uns für Flask entschieden. Die Implementierung in Python ist relativ einfach, jedoch treten teilweise Kompatibilitätsprobleme mit ROS 2 auf, und es entsteht ein erhöhter Performance-Bedarf durch das Threading.

7. *Verwendung einer USB-Kamera*:
Wir verwenden eine USB-Kamera, da diese eine gute Qualität und einen großen Winkel bietet. Ein Nachteil ist jedoch der hohe Stromverbrauch und der Fakt, dass man sie schlecht mit Servos bewegen kann.

8. *Wechsel auf OpenCV Video Stream Capture*:
Für das Videostream-Capturing nutzen wir OpenCV Video Stream Capture. Dadurch ist das direkte Ansprechen der Kamera in Python möglich. Allerdings läuft das Capturing permanent und verbraucht somit Performance.

9. *Wechsel auf YOLO (You Only Look Once)*:
Schließlich sind wir auf YOLO (You Only Look Once) umgestiegen, da es eine bessere Erkennungsgenauigkeit bietet. Der hohe Performance-Verbrauch und die relative geringe Verarbeitungsgeschwindigkeit sind jedoch Nachteile.

== Architekturmechanismen

=== Analysemechanismen

[discrete]
==== *Bilderfassung und Bildvorverarbeitung: UC1 - Roboter starten*
    * *Name*: Bilderfassung und -vorverarbeitung
    * *Beschreibung*: Mechanismus zur Erfassung von Bildern mit einer Kamera und deren Vorverarbeitung für Bildverarbeitung.
    * *Attribute*:
        ** *Quelle*: Kameraeingang (z.B. USB, CSI)
        ** *Auflösung*: Konfigurierbare Bildauflösung (z.B. 640x480, 1280x720)
        ** *Bildrate*: Konfigurierbare Bildrate (z.B. 30 FPS)
        ** *Vorverarbeitungsschritte*: Bildskalierung, Normalisierung, Rauschreduzierung

[discrete]
==== *Personenerkennung: UC1 - Roboter starten*
    * *Name*: Personenerkennung
    * *Beschreibung*: Mechanismus zur Erkennung und Identifizierung von Personen in erfassten Bildern.
    * *Attribute*:
        ** *Algorithmus*: Maschinelles Lernmodell (z.B. YOLO, SSD, Faster R-CNN)
        ** *Erkennungsgenauigkeit*: Konfigurierbarer Schwellenwert für Erkennungssicherheit
        ** *Erkennungsgeschwindigkeit*: Echtzeitverarbeitung

[discrete]
==== *Berechnung der Steuersignale: UC1 - Roboter starten*
    * *Name*: Berechnung der Steuersignale
    * *Beschreibung*: Mechanismus zur Berechnung von Steuersignalen für die Navigation des Roboters.
    * *Attribute*:
        ** *Winkelberechnung*: Berechnung des Winkels zur Verfolgung der erkannten Person
        ** *Geschwindigkeitsberechnung*: Berechnung der Geschwindigkeit zur Verfolgung der erkannten Person

[discrete]
==== *Verfolgung: UC1 - Roboter starten*
    * *Name*: Verfolgung
    * *Beschreibung*: Mechanismus zur Verfolgung der erkannten Person.
    * *Attribute*:
        ** *Schnittstelle zur Hardware*: Schnittstelle zur Steuerung der Roboterhardware

=== Entwurfsmechanismen

[discrete]
==== *Bilderfassung und -Vorverarbeitung: UC1 - Roboter starten*
    * *Name*: Bilderfassung und -Vorverarbeitung
    * *Beschreibung*: Mechanismus zur Erfassung und Vorverarbeitung von Bildern.
    * *Entwurf*:
        ** *Kameraschnittstelle*: Entwicklung einer benutzerdefinierten Kameraschnittstelle
        ** *Bildformat*: Definition eines Standardformats für Bilder (z.B. RGB)
        ** *Vorverarbeitungsschritte*: Implementierung von Funktionen zur Bildskalierung, Normalisierung und Rauschreduzierung

[discrete]
==== *Personenerkennung: UC1 - Roboter starten*
    * *Name*: Personenerkennung
    * *Beschreibung*: Mechanismus zur Erkennung und Identifizierung von Personen.
    * *Entwurf*:
        ** *Algorithmus*: Heraussuchen eines geeigneten ML-Modells
        ** *Erkennungsgenauigkeit*: Implementierung konfigurierbarer Schwellenwerte
        ** *Erkennungsgeschwindigkeit*: Echtzeitfähigkeit sicherstellen

[discrete]
==== *Berechnung der Steuersignale: UC1 - Roboter starten*
    * *Name*: Berechnung der Steuersignale
    * *Beschreibung*: Mechanismus zur Berechnung der Steuersignale zur Verfolgung der erkannten Person.
    * *Entwurf*:
        ** *Verfolgungsalgorithmus*: Entwicklung eines benutzerdefinierten Algorithmus
        ** *Leistung*: Effizient und zuverlässig mit niedriger Latenz

[discrete]
==== *Verfolgung: UC1 - Roboter starten*
    * *Name*: Verfolgung
    * *Beschreibung*: Mechanismus zur Verfolgung der erkannten Person.
    * *Entwurf*:
        ** *Steuerungsschnittstelle*: Entwicklung einer Schnittstelle zur Steuerung der Roboterhardware

=== Implementierungsmechanismen

[discrete]
==== *Bilderfassung und -vorverarbeitung: UC1 - Roboter starten*
    * *Struktur*:
        ** *ROS2-Paket*: Erstellen eines ROS2-Pakets
        ** *Knoten*: Entwicklung eines ROS2-Knotens zur Bilderfassung

[discrete]
==== *Personenerkennung: UC1 - Roboter starten*
    * *Struktur*:
        ** *ROS2-Paket*: Erstellen eines ROS2-Pakets
        ** *Knoten*: Entwicklung eines ROS2-Knotens zur Personenerkennung
        ** *Topics*: Nutzung eines Topics zur Veröffentlichung von Erkennungsergebnissen

[discrete]       
==== *Berechnung der Steuersignale: UC1 - Roboter starten*
    * *Struktur*:
        ** *ROS2-Paket*: Entwicklung eines ROS2-Pakets
        ** *Knoten*: Implementierung eines ROS2-Knotens zur Berechnung der Steuersignale
        ** *Topics*: Nutzung eines Topics zur Übermittlung von Verfolgungsinformationen
        ** *Schnittstellen*: Zugriff auf die Schnittstellen zur Steuerung der Roboterhardware

[discrete]
==== *Verfolgung: UC1 - Roboter starten*
    * *Struktur*:
        ** *Schnittstellen*: Entwicklung einer Schnittstelle zur Steuerung der Roboterhardware
        ** *Serielle Kommunikation*: Kommunikation mit der Roboterhardware über serielle Schnittstelle
        ** *Netzwerkkommunikation*: Kommunikation mit der Roboterhardware über Netzwerkverbindung


== Wesentliche Abstraktionen
=== Personen
- *Benutzer* - Person, die das System verwendet
- *Administrator* - Person, die das System konfiguriert und wartet

=== Roboter
- *Hardwareebene*: Raspberry Pi, Alphabot 2, Arduino Uno, Jetson Nano
- *Betriebssysteme*: ROS 2 Humble, Ubuntu Server 22.04.
- *Softwareebene*: Eigene entwickelte Software, Open-Source-Software.


== Schichten oder Architektur-Framework

image::ros-architecture.jpg[]

Unser Architektur-Framework wird durch die Verwendung von ROS II vorgegeben.

Als Middleware verwendet ROS II dabei die Data Distribution Service (link:https://automaticaddison.com/ros-2-architecture-overview/[DDS) Architektur]. 

Darauf baut dann der ROS II Client-Layer auf, welcher die Bibliotheken für die Verwendung von ROS zur Verfügung stelt. 

Zuletzt der Application-Layer, welcher die eigentliche Anwendung enthält.

== Architektursichten (Views)
=== Logische Sicht (C4-Modell): 

.System Context Diagramm
image::../../development/images/Design_Entwurf_7_C4_Context.drawio.png[]
.System Container Diagramm
image::../../development/images/Design_Entwurf_7_C4_Container.drawio.png[]
.System Component Diagramm
image::../../development/images/Design_Entwurf_7_C4_Component.drawio.png[]

=== Physische Sicht (Betriebssicht): 

Wir verwenden folgende Hardware zur Betreibung der Software:

* Prototyp 1
    ** Raspberry Pi 4 8GB
    ** Alphabot II

* Prototyp 2
    ** NVIDIA Jetson Nano
    ** Arduino Uno
    ** Adafruit Motor Shield v2.3
    ** 2 x DC Motoren
    ** USB-Webcam
    ** WLAN-Modul
    ** Batteriepack 4 AA Baterien
    ** Batteriepack 5V 4A
    ** Bluetooth HC-05 Modul (nur für Tests benötigt)

Des weiteren wird ein PC für das Ausführen der Software benötigt.

=== Use Cases:

image::../../requirements/images/use_case_model.drawio.png[]