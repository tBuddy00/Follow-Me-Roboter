= Dokumentation der HumanDetector-Klasse

== Einführung

Die Klasse `HumanDetector` ist darauf ausgelegt, Menschen in einem Videostream mithilfe von OpenCV zu erkennen und zu verfolgen. Sie verwendet den Haar Cascade Classifier, um Ganzkörpermuster zu identifizieren, und setzt einen Tracking-Mechanismus ein, um eine ausgewählte Person über die Bildsequenzen zu verfolgen. Die Klasse bietet außerdem Optionen, um verarbeitete Bilder anzuzeigen und zu speichern.

== Klassenattribute

* `name`: Ein String, der den Namen der Klasse `HumanDetector` repräsentiert.
* `bbox_person`: Der Begrenzungsrahmen der aktuell verfolgten Person.
* `frame_counter`: Eine ganze Zahl, die die Anzahl der verarbeiteten Frames verfolgt.
* `show_frame`: Ein boolscher Wert, der angibt, ob das verarbeitete Bild angezeigt werden soll.
* `save_image`: Ein boolscher Wert, der angibt, ob verarbeitete Bilder gespeichert werden sollen.
* `path_to_image`: Der Pfad, unter dem die Bilder gespeichert werden sollen.
* `full_body_cascade`: Eine Instanz des Haar Cascade Classifier, der auf die Erkennung von Mustern trainiert ist. Zu Testzwecken ist der Standardwert beim Initialisieren Gesichtserkennung.

== Konstruktor

* `__init__(self, show_frame=False, save_image=False, path_to_image="/home/ubuntu/image", xml_file="haarcascade_frontalface_default.xml")`: Initialisiert die Klasse `HumanDetector` mit den Standardattributen.
  - `show_frame`: Optionaler Parameter, um zu bestimmen, ob das verarbeitete Bild angezeigt werden soll.
  - `save_image`: Optionaler Parameter, um zu bestimmen, ob verarbeitete Bilder als Dateien gespeichert werden sollen.
  - `path_to_image`: Optionaler Parameter, um zu bestimmen, unter welchem Pfad die Bilder gespeichert werden sollen.
  - `xml_file`: Optionaler Parameter, um zu bestimmen, welche XML-Datei mit den Daten des Haar Cascade Classifiers genutzt werden soll.

== Methoden

=== `locate_person(self, frame)`

Lokalisiert eine Person im bereitgestellten Bild und ruft relevante Informationen ab.

Parameter:
* `frame`: Das Eingangsbild für die Personenerkennung.

Rückgabewert:
* Ein Tupel, das eine Liste mit Informationen über die lokalisierte Person und das verarbeitete Bild enthält.

=== `process_frame(self, frame)`

Verarbeitet jedes Bild, um Menschen zu erkennen und zu verfolgen, aktualisiert den Tracker und visualisiert Begrenzungsrahmen.

Parameter:
* `frame`: Das Eingangsbild für die Verarbeitung.

Rückgabewert:
* Das verarbeitete Bild mit visualisierten Begrenzungsrahmen.

=== `select_human(self, humans)`

Wählt den ersten erkannten Menschen für die Verfolgung aus und aktualisiert den Begrenzungsrahmen.

Parameter:
* `humans`: Eine Liste mit erkannten Koordinaten von Menschen.

=== `get_percentage_of_height(self, location, frame_height)`

Berechnet den Prozentsatz der Höhe der Person im Bild.

Parameter:
* `location`: Die Begrenzungsrahmen-Koordinaten der Person.
* `frame_height`: Die Höhe des Bildes.

Rückgabewert:
* Der Prozentsatz der Höhe der Person im Bild.

=== `draw_coordinate_system(self, frame)`

Zeichnet ein Koordinatensystem auf das Bild als Referenz.

Parameter:
* `frame`: Das Eingangsbild.

Rückgabewert:
* Das Bild mit dem eingezeichneten Koordinatensystem.

=== `cv_to_custom_coordinates(self, x_cv, y_cv, frame_width, frame_height)`

Konvertiert OpenCV-Koordinaten in benutzerdefinierte Koordinaten.

Parameter:
* `x_cv`: X-Koordinate in OpenCV.
* `y_cv`: Y-Koordinate in OpenCV.
* `frame_width`: Breite des Bildes.
* `frame_height`: Höhe des Bildes.

Rückgabewert:
* Ein Tupel mit den konvertierten X- und Y-Koordinaten.

== Beispielverwendung

[source,python]
----
# Beispielverwendung der HumanDetector-Klasse
if __name__ == "__main__":
    video_capture = cv2.VideoCapture(0)

    detector = HumanDetector(show_frame=True, save_image=True)
    frame = video_capture.read()[1]
    werte, verarbeitetes_bild = detector.locate_person(frame)
    print(werte)
----
